# frozen_string_literal: true

gem("google-cloud-storage", "~> 1.21")

require "google/cloud/storage"
require "active_support/notifications"
require "zlib"

module AppProfiler
  module Storage
    class GoogleCloudStorage < BaseStorage
      GOOGLE_SCOPE = "https://www.googleapis.com/auth/devstorage.read_write"

      class << self
        def upload(profile, _params = {})
          file = profile.file.open

          ActiveSupport::Notifications.instrument(
            "gcs_upload.app_profiler",
            file_size: file.size,
          ) do
            bucket.create_file(
              StringIO.new(gzipped_reader(file).read),
              gcs_filename(profile),
              content_type: "application/json",
              content_encoding: "gzip",
            )
          end
        end

        private

        def gcs_filename(profile)
          File.join(profile.context.to_s, profile.file.basename)
        end

        def bucket
          # The GCS gem requires the `storage.buckets.get`
          # permission to retrieve bucket details. We will set `skip_lookup` to
          # be true to skip this process.
          @bucket ||= Google::Cloud::Storage.new(
            credentials: credentials.presence,
            scope: GOOGLE_SCOPE,
            retries: 3,
          ).bucket(bucket_name, skip_lookup: true)
        end

        # We could compress everything at once using `ActiveSupport::Gzip.compress`.
        # Since we expect large files for profiles generated by stackprof,
        # compressing in chunks avoids keeping all of them in memory.
        def gzipped_reader(file)
          reader, writer = IO.pipe(binmode: true)
          Thread.new do
            writer.set_encoding("binary")
            gz = Zlib::GzipWriter.new(writer)
            # Default chunk size for gzip is 16384.
            gz.write(file.read(16384)) until file.eof?
            gz.close
          end
          reader
        end
      end
    end
  end
end
